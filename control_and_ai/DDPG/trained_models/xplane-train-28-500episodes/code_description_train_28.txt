train_ddpq_xplane.py

NUM_EPISODES = 500
SAVE_MOD = 10
SAVE_REWARD = True  # Export reward log as .xlsx
TRAIN_NUM = "28"
NAME = "xplane-train-" + TRAIN_NUM  # Model name

model_dir = 'D:/Programming/Python/rocketlander/control_and_ai/DDPG/trained_models/' + NAME

    for episode in range(1, NUM_EPISODES + 1):
        env.client.clearBuffer()
        old_state = None
        done = False
        total_reward = 0

        state = env.reset()
        state = util.normalize(state)
        max_steps = 500  # default max_steps = 500
        max_step_reached = False
        # time.sleep(2)

        for t in range(max_steps):
            old_state = state
            # infer an action
            action = agent.get_action(np.reshape(state, (1, obs_size)), not TEST)
            # print("action:", action)
            # take it
            env.client.clearBuffer()
            state, reward, done, _ = env.step(action[0])
            state = util.normalize(state)

            # penalty if max step is reached but have not landed
            if t >= max_steps:
                reward = -5000
                # max_step_reached = True

            # check again if crash happened but have not detected
            env.client.clearBuffer()
            try:
                crash = env.client.getDREF("sim/flightmodel2/misc/has_crashed")[0]
            except Exception as e:
                crash = 0
                env.client.clearBuffer()

            if crash == 1 and reward != -100000:
                print("starship crashed, detected outside step function")
                # print("crash 03:", self.crash)
                # done = True
                reward -= 100000
                done = True

            total_reward += reward
            # print("total reward:", total_reward)

            if not TEST:
                # update q vals
                agent.update(old_state, action[0], np.array(reward), state, done)

            if done:
                # print("Starship crashed/landed detected inside the train function")
                crash = 1
                while crash:
                    try:
                        crash = env.client.getDREF("sim/flightmodel2/misc/has_crashed")[0]
                    except Exception as e:
                        crash = 1
                        env.client.clearBuffer()

                # if crash > 0.0:
                #     time.sleep(20)

                break

        # if not max_step_reached:
        #     crash = 0
        #     while crash == 0:
        #         try:
        #             crash = env.client.getDREF("sim/flightmodel2/misc/has_crashed")
        #         except Exception as e:
        #             crash = 0
        #         time.sleep(1)

        agent.log_data(total_reward, episode)

        if episode % SAVE_MOD == 0 and not TEST:
            print('Saved model at episode', episode)
            agent.save_model(episode)
        if SAVE_REWARD:
            rew.append(total_reward)
            ep.append(episode)
        print("Episode:\t{0}\tReward:\t{1}".format(episode, total_reward))

_______________________________________________________________________________

xplane.py

    def _step(self, action):
        assert len(action) == 4

        # check if crash
        try:
            value = self.client.getDREF("sim/flightmodel2/misc/has_crashed")
            self.crash = value[0]
            # print("crash 02:", value)
        except Exception as e:
            self.crash = 0
            self.client.clearBuffer()

        if self.crash != 1.0 and self.crash != 0.0:
            print("Error in has_crash dataref. self.crash:", self.crash)
            self.client.clearBuffer()
            return np.array(self.previous_state), -10000, True, {}

        # print("self.crash:", self.crash)
        if self.crash == 1:
            print("starship crashed, detected inside step function")
            # print("crash 03:", self.crash)
            done = True
            reward = -100000
            self.crash = 0  # ?
            # print("crash 04:", self.crash)
            return np.array(self.state), reward, done, {}

        # Check for contact with the ground
        try:
            self.is_on_ground = self.client.getDREF('sim/flightmodel/failures/onground_any')[0]
        except Exception as e:
            self.is_on_ground = False
            self.client.clearBuffer()

        # print("is on ground:", self.is_on_ground)

        if self.is_on_ground != 1.0 and self.is_on_ground != 0.0:
            print("Error in is_on_ground dataref. self.is_on_ground:", self.is_on_ground)
            self.client.clearBuffer()
            return np.array(self.previous_state), -10000, True, {}

        # Shutdown all Engines upon contact with the ground
        if self.is_on_ground:
            action = [0, 0, 0, 0]

        elevator = action[0]
        aileron = action[1]
        rudder = action[2]
        throttle = action[3]
        # Execute the action
        control_values = [elevator, aileron, rudder, throttle]
        # print("Elevator:", action[0], "Aileron:", action[1], "Rudder:", action[2], "Throttle:", action[3])
        try:
            self.client.sendCTRL(control_values)
        except Exception as e:
            pass

        # control_values = [action[0], action[1], action[2], action[3], -988, -988]
        # try:
        #     self.client.sendCTRL(control_values)
        #
        #     # Gather Stats
        #     self.CTRL = self.client.getCTRL(0)
        #     elevator = self.CTRL[0]
        #     aileron = self.CTRL[1]
        #     rudder = self.CTRL[2]
        #     throttle = self.CTRL[3]
        # except Exception as e:
        #     elevator = 0
        #     aileron = 0
        #     rudder = 0
        #     throttle = 0
        #
        # self.action_history.append([elevator, aileron, rudder, throttle])
        self.action_history.append(control_values)

        # State Vector
        self.previous_state = self.state  # Keep a record of the previous state
        state = self.__generate_state()  # Generate state
        self.state = state  # Keep a record of the new state

        # set gear down if approaching ground
        if not self.is_on_ground and self.alti < 22.22 + 5:
            X = -998
            values = [X, X, X, X, X, X, 1]
            try:
                self.client.sendPOSI(values, 0)
            except Exception as e:
                pass

        # Rewards for reinforcement learning
        reward = self.__compute_rewards(state)
        done = False

        if self.is_on_ground:  # self.alti <= 22.22
            done = True
        # sleep(0.001)
        return np.array(state), reward, done, {}



    def __compute_rewards(self, state):
        # state = [self.alti, self.posx, self.posz, self.ver_vel, self.pitch, self.roll]
        reward = 0

        # negative reward for every step and altitude
        shaping = -20

        # positive reward if land inside the landing area
        if self.is_on_ground and not self.crash:
            if abs(self.ver_vel) < 1:
                if (-2.75 * state[1] - 98154.25) <= state[2] <= (-2.75 * state[1] - 97968.25):
                    if (0.35 * state[1] - 49946.15) <= state[2] <= (0.35 * state[1] - 49884.15):
                        shaping += 6000
                        print("starship landed inside the lander")
                    else:
                        # penalize if landed outside the landing area
                        shaping -= 6000
                        print("starship landed/crashed outside the lander")
                else:
                    shaping -= 6000
                    print("starship landed/crashed outside the lander")
        else:
            shaping -= np.sqrt(abs(state[0] - 22.22))
            # pass

        # give reward if approaching the ground and if the ver vel is low
        # if state[3] < 0:
        #     shaping = shaping + 3000 - state[0] + state[3] * 10

        # penalty for increasing vertical velocity
        if state[3] > 0:
            shaping -= state[3] * 1

        # penalty for error pitch and roll
        shaping -= ((abs(state[4]) * 40) + (abs(state[5]) * 100)) * 1

        # give positive reward if pitch and roll values are small
        # if abs(state[4]) > 1:
        #     shaping += 100
        #
        # if abs(state[5]) > 1:
        #     shaping += 100

        # keep track of previous reward shaping
        if self.prev_shaping is not None:
            reward = shaping - self.prev_shaping
        self.prev_shaping = shaping

        return reward